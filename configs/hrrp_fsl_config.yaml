# Configuration for Few-Shot HRRP Recognition (CMSA-HRRP - Adapter Version)

# --- Data Configuration ---
data:
  simulated_path: './datasets/simulated_hrrp'
  measured_path: './datasets/measured_hrrp'
  # Base and Novel classes (Ensure names match keys in semantic_descriptions.yaml -> class_mapping)
  base_classes: # 6 classes
    - 'EA-18G'
    - 'EP-3E'
    - 'F2'
    - 'F15'
    - 'F16'
    - 'F18'
  novel_classes: # 6 classes (3+3)
    - 'F22'
    - 'F35'
    - 'IDF'
    - '捕食者'
    - '幻影2000'
    - '全球鹰'
#    - 'an26'
#    - 'yar42'
#    - 'citation'
  target_length: 1000 # Processed HRRP length
  normalization: 'log_max' # 'max', 'log_max', 'none'

# --- Few-Shot Learning Configuration ---
fsl:
  n_way: 3          # N-way classification
  k_shot: 1         # K-shot learning (support samples per class) - Adjust as needed (e.g., 1 or 5)
  q_query: 15       # Query samples per class in each episode
  test_episodes: 600 # Number of episodes for testing
  classifier_temperature: 10.0 # Temperature for cosine classifier

# --- Model Configuration ---
model:
  # hrrp_encoder: # Configuration for the baseline 1D CNN
  #   output_dim: 512
  #   layers: [64, 128, 256, 512]
  #   kernel_size: 7
  adapter_1d_to_2d: # Configuration for the CMSA-HRRP adapter
    intermediate_dim: 2048 # MLP intermediate dimension
    # Other adapter params like activation could be added if HRPPtoPseudoImage is modified
  fusion_module:      # h_F (Used by both CMSA-HRRP and baseline 1D-CNN+Semantics)
    hidden_dim: 4096  # Example hidden dimension
    kappa: 0.5        # Default prototype fusion factor for testing (0: visual only, >0: enable fusion)
                      # test_fsl.py can override this or sweep values
  foundation_model:
    name: 'RemoteCLIP' # Currently only RemoteCLIP supported
    variant: 'RN50' # IMPORTANT: Change this for different scale models ('RN50', 'ViT-B-32', 'ViT-L-14')
    # IMPORTANT: Ensure these dimensions MATCH the chosen variant!
    # RN50: 1024, ViT-B-32: 512, ViT-L-14: 768
    visual_encoder_dim: 1024 # VLM visual encoder output dim (e.g., f_V output)
    text_encoder_dim: 1024   # VLM text encoder output dim (e.g., f_T output, z_T dim)

# --- Training Configuration ---
training:
  # Settings used for Adapter training (train_adapter.py),
  # SemAlign training (train_semalign_stage3.py),
  # and Baseline CNN training (train_baseline_cnn.py),
  # and Baseline Fusion training (train_fusion_1dcnn.py)
  # Adjust epochs/lr per stage if needed by loading specific sub-configs or adding more sections
  epochs: 10         # General epoch count, adjust per training script if needed
  batch_size: 128
  lr: 0.0001
  optimizer: 'Adam'
  loss_type: 'cosine' # Primarily for adapter alignment, others might use different losses (e.g., L1 for SemAlign, CE for baseline CNN)
  dropout_semalign: 0.2 # Dropout specific to SemAlign module training

# --- Semantics Configuration ---
semantics:
  # Path to the YAML file containing descriptions and mappings
  description_file: 'configs/semantic_descriptions.yaml'
  # generation settings control WHICH description to use and track the source LLM
  generation:
    # Source LLM name - MUST match the prefix used in description_file keys (e.g., 'gemini_2.5_pro'_generated)
    llm: 'gpt4' # Example: 'gemini_2.5_pro', 'gpt_4', 'manual' etc.
    # Type of text description to use for encoding
    text_type: 'llm_generated' # Options: 'name', 'detailed' (uses manual_detailed), 'llm_generated'
  # feature_path: # REMOVED - Path is now constructed dynamically in generate_semantics.py

# --- Paths Configuration ---
paths:
  # Base directories - specific files/subdirs constructed dynamically
  datasets_base: './datasets' # Contains simulated_path and measured_path subdirs (Corrected key)
  checkpoints: './checkpoints'
  logs: './logs'
  # Directory to save semantic features (used by generate_semantics.py)
  semantic_features_dir: './semantic_features'
  # base_centers_path: # REMOVED - Path is now constructed dynamically in compute_centers.py
  # foundation_model weights are expected in checkpoints/foundation_models/

# --- Other Hyperparameters ---
num_workers: 4
seed: 410073